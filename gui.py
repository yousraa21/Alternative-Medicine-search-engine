
# This file was generated by the Tkinter Designer by Parth Jadhav
# https://github.com/ParthJadhav/Tkinter-Designer


from pathlib import Path
from pathlib import Path
from tkinter import Tk, Canvas, Entry, Button, PhotoImage
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import math
import json
from tkinter import Text, END

# from tkinter import *
# Explicit imports to satisfy Flake8
from tkinter import Tk, Canvas, Entry, Text, Button, PhotoImage


OUTPUT_PATH = Path(__file__).parent
ASSETS_PATH = OUTPUT_PATH / Path(r"C:\Users\poste\Music\build\assets\frame0")


def relative_to_assets(path: str) -> Path:
    return ASSETS_PATH / Path(path)

########################################  expand query functions  ############################################################
def read_results_file(results_file):
    with open(results_file, 'r') as file:
        content = file.read()

    # Print the content for debugging
    print("Results File Content:", content)

    term_tfidf_scores = {}

    # Split the content into lines
    lines = content.split('\n')

    for line in lines:
        # Skip empty lines
        if not line.strip():
            continue

        # Split each line into term and associated document scores
        parts = line.split(':')
        if len(parts) == 2:
            term = parts[0].strip()
            doc_scores_str = parts[1].strip()
            doc_scores = eval(doc_scores_str)  # Safely evaluate the dictionary from the string
            term_tfidf_scores[term] = doc_scores

    return term_tfidf_scores



def read_preprocessed_document(collection_file, doc_number):
    with open(collection_file, 'r', encoding='utf-8-sig') as file:
        in_document = False
        current_document = ""
        doc_id_prefix = f'D{doc_number}:'

        for line in file:
            line = line.strip()

            if line.startswith("D") and in_document:
                # If a new document starts, break to the next document
                break

            if line.startswith(doc_id_prefix):
                in_document = True
                current_document += line[len(doc_id_prefix):] + "\n"
            elif in_document:
                # Append lines to the current document
                current_document += line + "\n"

    return current_document.strip()  # Return the preprocessed text without leading/trailing whitespaces

def calculate_tfidf(tf, df, N, document_scores):
    tfidf_scores = {}
    
    for doc_id, score in document_scores.items():
        tfidf_scores[doc_id] = tf * (1 + math.log(N / df)) * score

    return tfidf_scores
def get_top_terms(term_tfidf_list, n_t):
    term_tfidf_list.sort(key=lambda x: max(x[2].values()), reverse=True)
    top_terms = [term[0] for term in term_tfidf_list[:n_t]]
    return top_terms


def expand_query(original_query, n_d, n_t, results_file, collection_file):
    # Preprocess the original query
    preprocessed_query = preprocess_text(original_query)

    # Read the results file and extract the top n_d ranked documents
    results = read_results_file(results_file)
    top_documents = [doc_number for q, doc_number in results][:n_d]
    
    print("Top Documents:", top_documents)

    # Append the content of all the n_d documents together
    appended_content = ''
    for doc_number in top_documents:
        doc_content = read_preprocessed_document(collection_file, doc_number)
        if doc_content:
            appended_content += doc_content + ' '

    print("Appended Content:", appended_content)

    # Tokenize the content (replace this with your actual tokenization logic)
    terms = appended_content.split()

    # Calculate TF-IDF scores
    N = 10
    term_tfidf_list = [(term, terms.count(term), calculate_tfidf(terms.count(term), terms.count(term), N)) for term in set(terms)]

    print("TF-IDF List:", term_tfidf_list)

    # Get the top n_t terms with the highest TF-IDF scores
    top_expansion_terms = get_top_terms(term_tfidf_list, n_t)

    print("Top Expansion Terms:", top_expansion_terms)

    # Combine the original query and the expanded terms
    expanded_query = f"{preprocessed_query} {' '.join(top_expansion_terms)}"

    return expanded_query


def on_button_2_click():
    # Get the text entered in the Entry widget (user's query)
    original_query = entry_1.get()

    # Define the values for n_d and n_t
    n_d_value = 3  # Replace with the actual value you want to use
    n_t_value = 2   # Replace with the actual value you want to use

    # Define the file paths
    results_file_path = r"C:\Users\poste\Desktop\build\tfidf-result.txt"
    collection_file_path = r"C:\Users\poste\Desktop\build\processed_documents.txt"

    # Call expand_query function with the user's query and defined values and file paths
    result = expand_query(original_query, n_d_value, n_t_value, results_file_path, collection_file_path)
    print(result)



########################################expand query functions ############################################################



# Function for query text preprocessing
def preprocess_text(text, tokenize=True, normalize=True, remove_stopwords=True, stem=True):
    if tokenize:
        tokens = word_tokenize(text)
    else:
        tokens = text.split()

    if normalize:
        tokens = [word.lower() for word in tokens if word.isalpha()]

    if remove_stopwords:
        stop_words = set(stopwords.words('english'))
        tokens = [word for word in tokens if word not in stop_words]

    if stem:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(word) for word in tokens]

    return tokens

def load_inverted_index(file_path):
    with open(file_path, 'r') as json_file:
        inverted_index = json.load(json_file)
    return inverted_index

def calculate_tfidf_for_query(query_terms, inverted_index):
    query_tfidf = {}
    total_documents = len(inverted_index)  

    for term in query_terms:
        if term in inverted_index:
            # Calculate TF for the query term
            tf_query = query_terms.count(term)
            
            # Calculate IDF for the query term
            idf_query = math.log10(total_documents / len(inverted_index[term]))
            
            # Calculate TF-IDF score for the query term
            tfidf_query = tf_query * (1 + idf_query)
            
            query_tfidf[term] = tfidf_query

    return query_tfidf
    
def calculate_document_scores(query_tfidf_scores, inverted_index):
    # Calculate similarity scores for each document based on TF-IDF scores
    document_scores = {}

    for term, tfidf_query in query_tfidf_scores.items():
        if term in inverted_index:
            for document_info in inverted_index[term]:
                document_number = document_info[0].rstrip(':')
                positions = document_info[1]
                
                # Calculate a simple document score based on TF-IDF scores and positions
                score = tfidf_query * (1 + len(positions))
                
                # Accumulate scores for each document
                document_scores[document_number] = document_scores.get(document_number, 0) + score

    # Rank documents based on their accumulated scores
    ranked_documents = sorted(document_scores.items(), key=lambda x: x[1], reverse=True)

    return ranked_documents


def rank_documents(query_tfidf_scores, inverted_index):
    # Calculate the magnitude of the query vector
    query_magnitude = math.sqrt(sum(tfidf_score**2 for tfidf_score in query_tfidf_scores.values()))

    # Calculate cosine similarity between the query and each document
    document_scores = {}

    for term, tfidf_query in query_tfidf_scores.items():
        if term in inverted_index:
            for doc_id, tfidf_doc in inverted_index[term].items():
                document_scores[doc_id] = document_scores.get(doc_id, 0) + tfidf_query * tfidf_doc

    # Normalize the document scores by dividing by the magnitude of the document vectors
    for doc_id, score in document_scores.items():
        document_magnitude = math.sqrt(sum(tfidf_score**2 for tfidf_score in inverted_index[term][doc_id].values()))
        document_scores[doc_id] /= (query_magnitude * document_magnitude) if document_magnitude > 0 else 1

    # Rank documents based on cosine similarity scores
    ranked_documents = sorted(document_scores.items(), key=lambda x: x[1], reverse=True)

    return ranked_documents

def boolean_search(query, inverted_index, documents):
    # Preprocess the query
    query_terms = preprocess_text(query)

    # Separate the query into individual terms and operators
    terms_and_operators = []
    for term in query_terms:
        if term.lower() in ['and', 'or', 'not']:
            terms_and_operators.append(term.lower())
        else:
            terms_and_operators.append(term)

    # Initialize a set to store the final result
    result_set = set(range(1, len(documents) + 1))

    # Process the query using boolean operators
    current_operator = 'and'
    current_operand = set()

    for term_or_operator in terms_and_operators:
        if term_or_operator in ['and', 'or', 'not']:
            current_operator = term_or_operator
        else:
            term = term_or_operator

            # Get the set of documents containing the current term
            term_documents = set([int(doc_id) for doc_id, _ in inverted_index.get(term, [])])

            # Apply the current operator to update the result set
            if current_operator == 'and':
                current_operand.intersection_update(term_documents)
            elif current_operator == 'or':
                current_operand.update(term_documents)
            elif current_operator == 'not':
                current_operand.difference_update(term_documents)

    # Example (replace this with your logic):
    result = set()

    # Split the query into terms
    terms = query.split()

    # Perform boolean search
    for term in terms:
        if term in inverted_index:
            # Add documents containing the term to the result set
            result.update(inverted_index[term])

    return result

    
def read_documents(file_path):
    with open(file_path, 'r', encoding='utf-8-sig') as dataset_file:
        documents = {}
        in_document = False
        current_document = ""
        current_doc_id = None

        for line in dataset_file:
            line = line.strip()

            if line.startswith("D"):
                if in_document:
                    # If a new document starts, add the previous document to the dictionary
                    documents[current_doc_id] = current_document.strip()
                    current_document = ""

                # Start processing a new document
                current_doc_id = line.split(":")[0]
                in_document = True
            elif in_document:
                # Append lines to the current document
                current_document += line + "\n"

        # Add the last document to the dictionary
        if current_doc_id:
            documents[current_doc_id] = current_document.strip()

    return documents


#################### Callback function for when we click on search button
def on_button_click():
    # Get the text entered in the Entry widget
    query_text = entry_1.get()

    # Preprocess the entered query
    processed_query = preprocess_text(query_text)

    # Print the preprocessed query
    print("Preprocessed Query:", processed_query)

    # Load the inverted index from the JSON file
    inverted_index_path = r"C:\\Users\\poste\Desktop\\build\\inverted-index.json"
    inverted_index = load_inverted_index(inverted_index_path)
    # Example usage:
    dataset_path = r"C:\\Users\\poste\\Desktop\\build\\DATA-OF-ALTERNATIVE-MEDICINE-1.txt"
    documents = read_documents(dataset_path)

    # Check if the query contains boolean operators
    if any(op in processed_query for op in ['and', 'or', 'not']):
        # Perform boolean search
        result = boolean_search(processed_query, inverted_index, documents)

        # Clear previous content in the Text widget
        text_widget.delete(1.0, END)

        # Display the content of the documents in the boolean search result
        for doc_id in result:
            document_content = documents.get(doc_id, "Document not found")
            text_widget.insert(END, f"Document {doc_id}:\n{document_content}\n------\n")

        
        for doc_id in result:
            with open(dataset_path, 'r', encoding='utf-8-sig') as dataset_file:
                in_document = False
                current_document = ""
                doc_id_prefix = f'D{doc_id}:'

                for line in dataset_file:
                    line = line.strip()

                    if line.startswith("D") and in_document:
                        # If a new document starts, break to the next document
                        break

                    if line.startswith(doc_id_prefix):
                        in_document = True
                        current_document += line[len(doc_id_prefix):] + "\n"
                    elif in_document:
                        # Append lines to the current document
                        current_document += line + "\n"

                # Insert the content of the current document
                text_widget.insert(END, current_document + "\n------\n")

    else:
        # Calculate TF-IDF scores for the processed query
        query_tfidf_scores = calculate_tfidf_for_query(processed_query, inverted_index)

        # Retrieve and rank relevant documents
        ranked_documents = calculate_document_scores(query_tfidf_scores, inverted_index)

        # Print the TF-IDF scores for demonstration
        print("TF-IDF Scores:", query_tfidf_scores)
        # Print or use the ranked documents
        print("Ranked Documents:", ranked_documents)

        # Print or display the relevant documents content
        dataset_path = r"C:\\Users\\poste\\Desktop\\build\\DATA-OF-ALTERNATIVE-MEDICINE-1.txt"
        print("Top Ranked Documents:")

        # Clear previous content in the Text widget
        text_widget.delete(1.0, END)

        # Display the content of the top-ranked documents
        for doc_id, _ in ranked_documents:
            with open(dataset_path, 'r', encoding='utf-8-sig') as dataset_file:
                in_document = False
                current_document = ""
                doc_id_prefix = f'D{doc_id}:'

                for line in dataset_file:
                    line = line.strip()

                    if line.startswith("D") and in_document:
                        # If a new document starts, break to the next document
                        break

                    if line.startswith(doc_id_prefix):
                        in_document = True
                        current_document += line[len(doc_id_prefix):] + "\n"
                    elif in_document:
                        # Append lines to the current document
                        current_document += line + "\n"

                # Insert the content of the current document
                text_widget.insert(END, current_document + "\n------\n")

window = Tk()

window.geometry("649x541")
window.configure(bg = "#D7E5C9")


canvas = Canvas(
    window,
    bg = "#D7E5C9",
    height = 541,
    width = 649,
    bd = 0,
    highlightthickness = 0,
    relief = "ridge"
)

canvas.place(x = 0, y = 0)
entry_image_1 = PhotoImage(
    file=relative_to_assets("entry_1.png"))
entry_bg_1 = canvas.create_image(
    258.0,
    154.5,
    image=entry_image_1
)
entry_1 = Entry(
    bd=0,
    bg="#F2F4F0",
    fg="#000716",
    highlightthickness=0
)
entry_1.place(
    x=44.5,
    y=132.0,
    width=427.0,
    height=43.0
)

button_image_1 = PhotoImage(
    file=relative_to_assets("button_1.png"))
button_1 = Button(
    image=button_image_1,
    borderwidth=0,
    highlightthickness=0,
    command=on_button_click,
    relief="flat"
)
button_1.place(
    x=521.0,
    y=132.0,
    width=109.0,
    height=45.0
)

# Create a Text widget
text_widget = Text(
    window,
    wrap="word",
    bg="#F2F4F1",
    fg="#000716",
    font=("Arial", 10),
    relief="flat"
)
text_widget.place(
    x=47.0,
    y=194.0,
    width=554.0,
    height=254.0
)
canvas.create_rectangle(
    47.0,
    194.0,
    601.0,
    448.0,
    fill="#F2F4F1",
    outline="")

image_image_1 = PhotoImage(
    file=relative_to_assets("image_1.png"))
image_1 = canvas.create_image(
    532.0,
    66.0,
    image=image_image_1
)

canvas.create_text(
    150.0,
    54.0,
    anchor="nw",
    text="ALTERNATIVE MEDICINE",
    fill="#000000",
    font=("Italiana Regular", 21 * -1)
)

button_image_2 = PhotoImage(
    file=relative_to_assets("button_2.png"))
button_2 = Button(
    image=button_image_2,
    borderwidth=0,
    highlightthickness=0,
    command=on_button_2_click,
    relief="flat"
)
button_2.place(
    x=226.0,
    y=473.0,
    width=196.0,
    height=37.0
)
window.resizable(False, False)
window.mainloop()

